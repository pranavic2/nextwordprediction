{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80630654",
   "metadata": {},
   "source": [
    "# Next Word Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27f449",
   "metadata": {},
   "source": [
    "importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b57ca008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458d154",
   "metadata": {},
   "source": [
    "get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa7011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=requests.get(\"http://www.gutenberg.org/cache/epub/5200/pg5200.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f1f428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Metamorphosis, by Franz Kafka\\r\\nTranslated by David Wyllie.\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\r\\nre-use it under the terms of the Project Gutenberg License included\\r\\nwith this eBook or online at www.gutenberg.org\\r\\n\\r\\n** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **\\r\\n**     Please follow the copyright guidelines in this file.     **\\r\\n\\r\\n\\r\\nTitle: Metamorphosis\\r\\n\\r\\nAuthor: Franz Kafka\\r\\n\\r\\nTranslator: David Wyllie\\r\\n\\r\\nRelease Date: August 16, 2005 [EBook #5200]\\r\\nFirst posted: May 13, 2002\\r\\nLast updated: May 20, 2012\\r\\n\\r\\nLanguage: English\\r\\n\\r\\n\\r\\n*** START OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCopyright (C) 2002 David Wyllie.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n  Metamorphosis\\r\\n  Franz Kafka\\r\\n\\r\\nTranslated by David Wyllie\\r\\n\\r\\n\\r\\n\\r\\nI\\r\\n\\r\\n\\r\\nOne morning, when Gregor Samsa woke from troubled dreams, he found\\r\\nhimself transformed in his bed into a horrible vermin.  He lay on\\r\\nhis armour-like back, and if he lifted his head a little he could\\r\\nsee his brown belly, slightly domed and divided by arches into stiff\\r\\nsections.  The bedding was hardly able to cover it and seemed ready\\r\\nto slide off any moment.  His many legs, pitifully thin compared\\r\\nwith the size of the rest of him, waved about helplessly as he\\r\\nlooked.\\r\\n\\r\\n\"What\\'s happened to me?\" he thought.  It wasn\\'t a dream.  His room,\\r\\na proper human room although a little too small, lay peacefully\\r\\nbetwee'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[:1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e744b9dd",
   "metadata": {},
   "source": [
    "Split the data set into lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23c01104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Metamorphosis, by Franz Kafka\\r'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = response.text.split('\\n')\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f073ea0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'away from the bed, bend down with the load and then be patient and\\r'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[253:]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d8f4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2110"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8fd67",
   "metadata": {},
   "source": [
    "Right now we have a list of the lines in the data. Now we are going to join all the lines and create a long string consisting of the data in continuous format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d76b49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'away from the bed, bend down with the load and then be patient and\\r careful as he swang over onto the floor, where, hopefully, the\\r little legs would find a use.  Should he really call for help\\r though, even apart from the fact that all the doors were locked?\\r Despite all the difficulty he was in, he could not suppress a smile\\r at this thought.\\r \\r After a while he had already moved so far across that it would have\\r been hard for him to keep his balance if he rocked too hard.  The\\r time was now ten past seven and he would have to make a final\\r decision very soon.  Then there was a ring at the door of the flat.\\r \"That\\'ll be someone from work\", he said to himself, and froze very\\r still, although his little legs only became all the more lively as\\r they danced around.  For a moment everything remained quiet.\\r \"They\\'re not opening the door\", Gregor said to himself, caught in\\r some nonsensical hope.  But then of course, the maid\\'s firm steps\\r went to the door as ever and opened it.  Gregor on'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \" \".join(data)\n",
    "data[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c5416",
   "metadata": {},
   "source": [
    "we can see that after passing data to clean_text we get the data in the required format without punctuations and special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db550dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['away', 'from', 'the', 'bed', 'bend', 'down', 'with', 'the', 'load', 'and', 'then', 'be', 'patient', 'and', 'careful', 'as', 'he', 'swang', 'over', 'onto', 'the', 'floor', 'where', 'hopefully', 'the', 'little', 'legs', 'would', 'find', 'a', 'use', 'should', 'he', 'really', 'call', 'for', 'help', 'though', 'even', 'apart', 'from', 'the', 'fact', 'that', 'all', 'the', 'doors', 'were', 'locked', 'despite']\n"
     ]
    }
   ],
   "source": [
    "def clean_text(doc):\n",
    "  tokens = doc.split()\n",
    "  table = str.maketrans('', '', string.punctuation)\n",
    "  tokens = [w.translate(table) for w in tokens]\n",
    "  tokens = [word for word in tokens if word.isalpha()]\n",
    "  tokens = [word.lower() for word in tokens]\n",
    "  return tokens\n",
    "\n",
    "tokens = clean_text(data)\n",
    "print(tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e049daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22607"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff3e5d1",
   "metadata": {},
   "source": [
    "we are going to use a set of previous words to predict the next word in the sentence. To be precise we are going to use a set of 50 words to predict the 51st word. Hence we are going to divide our data in chunks of 51 words and at the last we will separate the last word from every line. We are going to limit our dataset to 200000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d495b48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22556\n"
     ]
    }
   ],
   "source": [
    "length = 50 + 1\n",
    "lines = []\n",
    "\n",
    "for i in range(length, len(tokens)):\n",
    "  seq = tokens[i-length:i]\n",
    "  line = ' '.join(seq)\n",
    "  lines.append(line)\n",
    "  if i > 200000:\n",
    "    break\n",
    "\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8d95e6",
   "metadata": {},
   "source": [
    "# Build LSTM Model and Prepare X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ab0298",
   "metadata": {},
   "source": [
    "import all the necessary libraries used to pre-process the data and create the layers of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88eaa9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9cf0ae",
   "metadata": {},
   "source": [
    "We are going to create a unique numerical token for each unique word in the dataset.fit_on_texts() updates internal vocabulary based on a list of texts. texts_to_sequences() transforms each text in texts to a sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c52d89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6bd39e",
   "metadata": {},
   "source": [
    "sequences containes a list of integer values created by tokenizer. Each line in sequences has 51 words. Now we will split each line such that the first 50 words are in X and the last word is in y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f31c4d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 103,   29,    1,  245, 2883,   98,   14,    1, 1435,    3,   48,\n",
       "         30,  618,    3,  756,   13,    6, 1434,  107,  165,    1,  149,\n",
       "         86, 2880,    1,   78,  225,   21,  530,   12,  156,  193,    6,\n",
       "        142,  754,   17,  180,  116,   49, 1433,   29,    1,  753,   11,\n",
       "         26,    1,  455,   58,  617,  329])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:, :-1], sequences[:,-1]\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ade0d",
   "metadata": {},
   "source": [
    "vocab_size contains all the uniques words in the dataset. tokenizer.word_index gives the mapping of each unique word to its numerical equivalent. Hence len() of tokenizer.word_index gives the vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a842328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980d73a7",
   "metadata": {},
   "source": [
    "to_categorical() converts a class vector (integers) to binary class matrix. num_classes is the total number of classes which is vocab_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6fd74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be34bc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = X.shape[1]\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b7dfb",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52df29bf",
   "metadata": {},
   "source": [
    "A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ac0d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "573788aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 50)            144250    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2885)              291385    \n",
      "=================================================================\n",
      "Total params: 586,535\n",
      "Trainable params: 586,535\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86f0e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bc5c9a",
   "metadata": {},
   "source": [
    "After compiling the model we will now train the model using model.fit() on the training dataset. We will use 100 epochs to train the model. An epoch is an iteration over the entire x and y data provided. batch_size is the number of samples per gradient update i.e. the weights will be updates after 256 training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "deff909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "89/89 [==============================] - 32s 286ms/step - loss: 6.6286 - accuracy: 0.0533\n",
      "Epoch 2/100\n",
      "89/89 [==============================] - 28s 312ms/step - loss: 6.1863 - accuracy: 0.0540\n",
      "Epoch 3/100\n",
      "89/89 [==============================] - 29s 324ms/step - loss: 6.1646 - accuracy: 0.0540\n",
      "Epoch 4/100\n",
      "89/89 [==============================] - 30s 335ms/step - loss: 6.0548 - accuracy: 0.0540\n",
      "Epoch 5/100\n",
      "89/89 [==============================] - 32s 358ms/step - loss: 5.9604 - accuracy: 0.0546\n",
      "Epoch 6/100\n",
      "89/89 [==============================] - 29s 333ms/step - loss: 5.8097 - accuracy: 0.0606\n",
      "Epoch 7/100\n",
      "89/89 [==============================] - 31s 354ms/step - loss: 5.7121 - accuracy: 0.0674\n",
      "Epoch 8/100\n",
      "89/89 [==============================] - 31s 345ms/step - loss: 5.6420 - accuracy: 0.0727\n",
      "Epoch 9/100\n",
      "89/89 [==============================] - 34s 384ms/step - loss: 5.5737 - accuracy: 0.0771\n",
      "Epoch 10/100\n",
      "89/89 [==============================] - 34s 381ms/step - loss: 5.5113 - accuracy: 0.0814\n",
      "Epoch 11/100\n",
      "89/89 [==============================] - 32s 361ms/step - loss: 5.4571 - accuracy: 0.0869\n",
      "Epoch 12/100\n",
      "89/89 [==============================] - 32s 359ms/step - loss: 5.4046 - accuracy: 0.0900\n",
      "Epoch 13/100\n",
      "89/89 [==============================] - 32s 361ms/step - loss: 5.3515 - accuracy: 0.0951\n",
      "Epoch 14/100\n",
      "89/89 [==============================] - 35s 391ms/step - loss: 5.2971 - accuracy: 0.0994\n",
      "Epoch 15/100\n",
      "89/89 [==============================] - 31s 353ms/step - loss: 5.2420 - accuracy: 0.1012\n",
      "Epoch 16/100\n",
      "89/89 [==============================] - 31s 344ms/step - loss: 5.1879 - accuracy: 0.1050\n",
      "Epoch 17/100\n",
      "89/89 [==============================] - 31s 348ms/step - loss: 5.1422 - accuracy: 0.1076\n",
      "Epoch 18/100\n",
      "89/89 [==============================] - 31s 351ms/step - loss: 5.0895 - accuracy: 0.1127\n",
      "Epoch 19/100\n",
      "89/89 [==============================] - 31s 349ms/step - loss: 5.0384 - accuracy: 0.1159\n",
      "Epoch 20/100\n",
      "89/89 [==============================] - 31s 349ms/step - loss: 4.9907 - accuracy: 0.1197\n",
      "Epoch 21/100\n",
      "89/89 [==============================] - 31s 345ms/step - loss: 4.9425 - accuracy: 0.1231\n",
      "Epoch 22/100\n",
      "89/89 [==============================] - 33s 371ms/step - loss: 4.8975 - accuracy: 0.1242\n",
      "Epoch 23/100\n",
      "89/89 [==============================] - 31s 348ms/step - loss: 4.8493 - accuracy: 0.1282\n",
      "Epoch 24/100\n",
      "89/89 [==============================] - 31s 350ms/step - loss: 4.7985 - accuracy: 0.1312\n",
      "Epoch 25/100\n",
      "89/89 [==============================] - 31s 350ms/step - loss: 4.7511 - accuracy: 0.1360\n",
      "Epoch 26/100\n",
      "89/89 [==============================] - 31s 353ms/step - loss: 4.7086 - accuracy: 0.1386\n",
      "Epoch 27/100\n",
      "89/89 [==============================] - 32s 355ms/step - loss: 4.6627 - accuracy: 0.1423\n",
      "Epoch 28/100\n",
      "89/89 [==============================] - 32s 360ms/step - loss: 4.6216 - accuracy: 0.1446\n",
      "Epoch 29/100\n",
      "89/89 [==============================] - 33s 368ms/step - loss: 4.5774 - accuracy: 0.1502\n",
      "Epoch 30/100\n",
      "89/89 [==============================] - 28s 317ms/step - loss: 4.5345 - accuracy: 0.1517\n",
      "Epoch 31/100\n",
      "89/89 [==============================] - 29s 326ms/step - loss: 4.4967 - accuracy: 0.1579\n",
      "Epoch 32/100\n",
      "89/89 [==============================] - 30s 339ms/step - loss: 4.4517 - accuracy: 0.1585\n",
      "Epoch 33/100\n",
      "89/89 [==============================] - 30s 336ms/step - loss: 4.4139 - accuracy: 0.1627\n",
      "Epoch 34/100\n",
      "89/89 [==============================] - 33s 369ms/step - loss: 4.3756 - accuracy: 0.1644\n",
      "Epoch 35/100\n",
      "89/89 [==============================] - 33s 371ms/step - loss: 4.3356 - accuracy: 0.1671\n",
      "Epoch 36/100\n",
      "89/89 [==============================] - 33s 368ms/step - loss: 4.2984 - accuracy: 0.1655\n",
      "Epoch 37/100\n",
      "89/89 [==============================] - 34s 379ms/step - loss: 4.2631 - accuracy: 0.1698\n",
      "Epoch 38/100\n",
      "89/89 [==============================] - 34s 379ms/step - loss: 4.2237 - accuracy: 0.1719\n",
      "Epoch 39/100\n",
      "89/89 [==============================] - 34s 379ms/step - loss: 4.1897 - accuracy: 0.1750\n",
      "Epoch 40/100\n",
      "89/89 [==============================] - 34s 381ms/step - loss: 4.1546 - accuracy: 0.1766\n",
      "Epoch 41/100\n",
      "89/89 [==============================] - 34s 376ms/step - loss: 4.1202 - accuracy: 0.1791\n",
      "Epoch 42/100\n",
      "89/89 [==============================] - 34s 380ms/step - loss: 4.0842 - accuracy: 0.1827\n",
      "Epoch 43/100\n",
      "89/89 [==============================] - 33s 371ms/step - loss: 4.0563 - accuracy: 0.1830\n",
      "Epoch 44/100\n",
      "89/89 [==============================] - 33s 366ms/step - loss: 4.0208 - accuracy: 0.1851\n",
      "Epoch 45/100\n",
      "89/89 [==============================] - 33s 373ms/step - loss: 3.9841 - accuracy: 0.1880\n",
      "Epoch 46/100\n",
      "89/89 [==============================] - 34s 383ms/step - loss: 4.0119 - accuracy: 0.1854\n",
      "Epoch 47/100\n",
      "89/89 [==============================] - 34s 379ms/step - loss: 4.6439 - accuracy: 0.1425\n",
      "Epoch 48/100\n",
      "89/89 [==============================] - 34s 379ms/step - loss: 4.2285 - accuracy: 0.1657\n",
      "Epoch 49/100\n",
      "89/89 [==============================] - 34s 380ms/step - loss: 4.0601 - accuracy: 0.1766\n",
      "Epoch 50/100\n",
      "89/89 [==============================] - 33s 375ms/step - loss: 3.9604 - accuracy: 0.1887\n",
      "Epoch 51/100\n",
      "89/89 [==============================] - 34s 377ms/step - loss: 3.9000 - accuracy: 0.1937\n",
      "Epoch 52/100\n",
      "89/89 [==============================] - 34s 381ms/step - loss: 3.8429 - accuracy: 0.1990\n",
      "Epoch 53/100\n",
      "89/89 [==============================] - 34s 380ms/step - loss: 3.8056 - accuracy: 0.2029\n",
      "Epoch 54/100\n",
      "89/89 [==============================] - 34s 380ms/step - loss: 3.8391 - accuracy: 0.1978\n",
      "Epoch 55/100\n",
      "89/89 [==============================] - 34s 381ms/step - loss: 3.8186 - accuracy: 0.2002\n",
      "Epoch 56/100\n",
      "89/89 [==============================] - 34s 377ms/step - loss: 3.7560 - accuracy: 0.2077\n",
      "Epoch 57/100\n",
      "89/89 [==============================] - 34s 379ms/step - loss: 3.7023 - accuracy: 0.2109\n",
      "Epoch 58/100\n",
      "89/89 [==============================] - 34s 381ms/step - loss: 3.6516 - accuracy: 0.2193\n",
      "Epoch 59/100\n",
      "89/89 [==============================] - 34s 380ms/step - loss: 3.6059 - accuracy: 0.2254\n",
      "Epoch 60/100\n",
      "89/89 [==============================] - 35s 393ms/step - loss: 3.5610 - accuracy: 0.2298\n",
      "Epoch 61/100\n",
      "89/89 [==============================] - 34s 381ms/step - loss: 3.5158 - accuracy: 0.2357\n",
      "Epoch 62/100\n",
      "89/89 [==============================] - 34s 381ms/step - loss: 3.4702 - accuracy: 0.2402\n",
      "Epoch 63/100\n",
      "89/89 [==============================] - 34s 379ms/step - loss: 3.4297 - accuracy: 0.2471\n",
      "Epoch 64/100\n",
      "89/89 [==============================] - 34s 381ms/step - loss: 3.3787 - accuracy: 0.2532\n",
      "Epoch 65/100\n",
      "89/89 [==============================] - 34s 383ms/step - loss: 3.3360 - accuracy: 0.2606\n",
      "Epoch 66/100\n",
      "89/89 [==============================] - 34s 382ms/step - loss: 3.2913 - accuracy: 0.2649\n",
      "Epoch 67/100\n",
      "89/89 [==============================] - 34s 379ms/step - loss: 3.2408 - accuracy: 0.2743\n",
      "Epoch 68/100\n",
      "89/89 [==============================] - 34s 382ms/step - loss: 3.1928 - accuracy: 0.2818\n",
      "Epoch 69/100\n",
      "89/89 [==============================] - 34s 380ms/step - loss: 3.1544 - accuracy: 0.2872\n",
      "Epoch 70/100\n",
      "89/89 [==============================] - 34s 379ms/step - loss: 3.1073 - accuracy: 0.2967\n",
      "Epoch 71/100\n",
      "89/89 [==============================] - 34s 382ms/step - loss: 3.0578 - accuracy: 0.3033\n",
      "Epoch 72/100\n",
      "89/89 [==============================] - 34s 379ms/step - loss: 3.0159 - accuracy: 0.3102\n",
      "Epoch 73/100\n",
      "89/89 [==============================] - 34s 381ms/step - loss: 2.9814 - accuracy: 0.3133\n",
      "Epoch 74/100\n",
      "89/89 [==============================] - 34s 381ms/step - loss: 2.9322 - accuracy: 0.3246\n",
      "Epoch 75/100\n",
      "89/89 [==============================] - 33s 372ms/step - loss: 2.8932 - accuracy: 0.3305\n",
      "Epoch 76/100\n",
      "89/89 [==============================] - 34s 384ms/step - loss: 2.8588 - accuracy: 0.3352\n",
      "Epoch 77/100\n",
      "89/89 [==============================] - 34s 380ms/step - loss: 2.8155 - accuracy: 0.3459\n",
      "Epoch 78/100\n",
      "89/89 [==============================] - 34s 382ms/step - loss: 2.7867 - accuracy: 0.3518\n",
      "Epoch 79/100\n",
      "89/89 [==============================] - 34s 381ms/step - loss: 2.8494 - accuracy: 0.3391\n",
      "Epoch 80/100\n",
      "89/89 [==============================] - 34s 380ms/step - loss: 2.7980 - accuracy: 0.3495\n",
      "Epoch 81/100\n",
      "89/89 [==============================] - 34s 379ms/step - loss: 2.9486 - accuracy: 0.3259\n",
      "Epoch 82/100\n",
      "89/89 [==============================] - 34s 382ms/step - loss: 2.7581 - accuracy: 0.3537\n",
      "Epoch 83/100\n",
      "89/89 [==============================] - 34s 379ms/step - loss: 2.7084 - accuracy: 0.3623\n",
      "Epoch 84/100\n",
      "89/89 [==============================] - 34s 380ms/step - loss: 2.6768 - accuracy: 0.3687\n",
      "Epoch 85/100\n",
      "89/89 [==============================] - 34s 380ms/step - loss: 2.6200 - accuracy: 0.3807\n",
      "Epoch 86/100\n",
      "89/89 [==============================] - 34s 382ms/step - loss: 2.6001 - accuracy: 0.3861\n",
      "Epoch 87/100\n",
      "89/89 [==============================] - 34s 382ms/step - loss: 2.5829 - accuracy: 0.3881\n",
      "Epoch 88/100\n",
      "89/89 [==============================] - 34s 383ms/step - loss: 2.6642 - accuracy: 0.3777\n",
      "Epoch 89/100\n",
      "89/89 [==============================] - 34s 381ms/step - loss: 2.6497 - accuracy: 0.3776\n",
      "Epoch 90/100\n",
      "89/89 [==============================] - 34s 381ms/step - loss: 2.7969 - accuracy: 0.3494\n",
      "Epoch 91/100\n",
      "89/89 [==============================] - 34s 385ms/step - loss: 2.7195 - accuracy: 0.3664\n",
      "Epoch 92/100\n",
      "89/89 [==============================] - 34s 382ms/step - loss: 2.8789 - accuracy: 0.3356\n",
      "Epoch 93/100\n",
      "89/89 [==============================] - 34s 384ms/step - loss: 2.9671 - accuracy: 0.3139\n",
      "Epoch 94/100\n",
      "89/89 [==============================] - 34s 384ms/step - loss: 2.8647 - accuracy: 0.3333\n",
      "Epoch 95/100\n",
      "89/89 [==============================] - 34s 385ms/step - loss: 2.8068 - accuracy: 0.3487\n",
      "Epoch 96/100\n",
      "89/89 [==============================] - 34s 382ms/step - loss: 2.7667 - accuracy: 0.3527\n",
      "Epoch 97/100\n",
      "89/89 [==============================] - 34s 383ms/step - loss: 2.8761 - accuracy: 0.3300\n",
      "Epoch 98/100\n",
      "89/89 [==============================] - 34s 381ms/step - loss: 2.8692 - accuracy: 0.3283\n",
      "Epoch 99/100\n",
      "89/89 [==============================] - 34s 382ms/step - loss: 2.7949 - accuracy: 0.3441\n",
      "Epoch 100/100\n",
      "89/89 [==============================] - 34s 380ms/step - loss: 2.7516 - accuracy: 0.3507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2979c8d47f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size = 256, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c63c47f",
   "metadata": {},
   "source": [
    "We are now going to generate words using the model. For this we need a set of 50 words to predict the 51st word. So we are taking a random line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15b45774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'condition seemed serious enough to remind even his father that gregor despite his current sad and revolting form was a family member who could not be treated as an enemy on the contrary as a family there was a duty to swallow any revulsion for him and to be patient just'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text=lines[12343]\n",
    "seed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55da83e5",
   "metadata": {},
   "source": [
    "generate_text_seq() generates n_words number of words after the given seed_text. We are going to pre-process the seed_text before predicting. We are going to encode the seed_text using the same encoding used for encoding the training data. Then we are going to convert the seed_textto 50 words by using pad_sequences(). Now we will predict using model.predict_classes(). After that we will search the word in tokenizer using the index in y_predict. Finally we will append the predicted word to seed_text and text and repeat the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "367d7b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_seq(model, tokenizer, text_seq_length, seed_text, n_words):\n",
    "  text = []\n",
    "\n",
    "  for _ in range(n_words):\n",
    "    encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    encoded = pad_sequences([encoded], maxlen = text_seq_length, truncating='pre')\n",
    "\n",
    "    y_predict = model.predict_classes(encoded)\n",
    "\n",
    "    predicted_word = ''\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "      if index == y_predict:\n",
    "        predicted_word = word\n",
    "        break\n",
    "    seed_text = seed_text + ' ' + predicted_word\n",
    "    text.append(predicted_word)\n",
    "  return ' '.join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860b552",
   "metadata": {},
   "source": [
    "We can see that the next 100 words are predicted by the model for the seed_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cab9ae07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Drivers\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the first word to him the door and wipe himself on the floor he had been knocked at all the door and was already begun to speak to him and the door and slid down in front of the couch shivering the living room and pressed up against the door to him he had been working for them and smoking though the practise he was already able to speak to him mixed from by hope that he was stretching him out the door to bear the door to bear the door to him he had woke to attract the door'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_seq(model, tokenizer, seq_length, seed_text, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a01763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
